
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>@ijin</title>
  <meta name="author" content="Michael H. Oshita">

  
  <meta name="description" content="以前、「非ELBなAutoscalingによる自動復旧」でインスタンスの自動復旧の挙動をテストしました。
障害が発生したサーバをterminateし、新サーバをstartしてリプレースする仕組みはまさに最近話題のImmutable Infrastructureですね。
CDP的には「Server &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://ijin.github.com/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/data-table.css" rel="stylesheet" type="text/css" />
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="@ijin" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-33110241-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">@ijin</a></h1>
  
    <h2>[Michael H. Oshita]</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:ijin.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/12/14/self-healing-with-non-elb-autoscaling2/">Autoscalingによる自動復旧(Immutable Infrastucture)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-12-14T23:40:00+09:00" pubdate data-updated="true">Dec 14<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>以前、「<a href="/blog/2013/02/08/self-healing-with-non-elb-autoscaling/">非ELBなAutoscalingによる自動復旧</a>」でインスタンスの自動復旧の挙動をテストしました。
障害が発生したサーバをterminateし、新サーバをstartしてリプレースする仕組みはまさに最近話題のImmutable Infrastructureですね。
CDP的には「<a href="http://aws.clouddesignpattern.org/index.php/CDP:Server_Swapping%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3">Server Swappingパターン</a>」が一番近いですが、今後はImmutable分類もあっても良いような気がします。</p>

<p>前回はAuto Scalingがインスタンス障害を検知してリプレースするまでのタイムラグが約20分だと分かりました。
本日、インスタンスの状態をチェックするEC2 Status Checkが1分間隔になった（以前は5分間隔）と<a href="https://forums.aws.amazon.com/ann.jspa?annID=2266">発表</a>されたので、
これによってタイムラグが短縮されたかを検証してみます。</p>

<h3>設定</h3>

<p>手順は前回と一緒なので省略</p>

<h3>自動復旧</h3>

<p>通信を遮断し、Status Check Failを発動させる</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ubuntu@ip-10-123-32-180:~$ date; sudo ifdown eth0
</span><span class='line'>Sat Dec 14 14:19:19 UTC 2013
</span><span class='line'>Write failed: Broken pipe</span></code></pre></td></tr></table></div></figure>


<p>EC2のStatus Checkを流す</p>

<pre><code>while true; do date; aws ec2 describe-instance-status --instance-ids i-b03788b5 --query 'InstanceStatuses[*].InstanceStatus' --output text ; echo ; sleep 10; done
</code></pre>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Sat Dec 14 23:22:05 JST 2013
</span><span class='line'>ok
</span><span class='line'>DETAILS reachability    passed
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:22:16 JST 2013
</span><span class='line'>ok
</span><span class='line'>DETAILS reachability    passed
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:22:27 JST 2013
</span><span class='line'>impaired
</span><span class='line'>DETAILS 2013-12-14T14:22:00.000Z        reachability    failed
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:22:37 JST 2013
</span><span class='line'>impaired
</span><span class='line'>DETAILS 2013-12-14T14:22:00.000Z        reachability    failed</span></code></pre></td></tr></table></div></figure>


<p>約3分でStatus異常が検知されました。</p>

<p><img src="https://lh5.googleusercontent.com/-pabfvBU5fW0/Uqx4SEmBYLI/AAAAAAAAA2E/abCLUwoESds/w734-h154-no/Instance_status_check_2013-12-14+at+11.34.27+PM.png"></p>

<p>Auto ScalingのHealthStatusを流す</p>

<pre><code>while true; do date; aws autoscaling describe-auto-scaling-instances --query 'AutoScalingInstances[*].HealthStatus' --output text; echo; sleep 10; done
</code></pre>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Sat Dec 14 23:38:06 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "InService", 
</span><span class='line'>        "Health": "HEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:38:17 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "InService", 
</span><span class='line'>        "Health": "UNHEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:38:28 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "InService", 
</span><span class='line'>        "Health": "UNHEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:38:38 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "Terminating", 
</span><span class='line'>        "Health": "UNHEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:38:49 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "Terminating", 
</span><span class='line'>        "Health": "UNHEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:38:59 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "Terminating", 
</span><span class='line'>        "Health": "UNHEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span><span class='line'>
</span><span class='line'>Sat Dec 14 23:39:10 JST 2013
</span><span class='line'>[
</span><span class='line'>    {
</span><span class='line'>        "State": "Pending", 
</span><span class='line'>        "Health": "HEALTHY", 
</span><span class='line'>        "ID": "i-4cc7a849"
</span><span class='line'>    }, 
</span><span class='line'>    {
</span><span class='line'>        "State": "Terminating", 
</span><span class='line'>        "Health": "UNHEALTHY", 
</span><span class='line'>        "ID": "i-b03788b5"
</span><span class='line'>    }
</span><span class='line'>]
</span></code></pre></td></tr></table></div></figure>


<p>やっとAuto Scalingの方でも異常検知。</p>

<p><img src="https://lh4.googleusercontent.com/-8Dj_s0mm9I8/Uqx4R1-Wl9I/AAAAAAAAA2I/laUZmwhUw8o/w873-h151-no/Autoscaling_health_2013-12-14+at+11.57.42+PM.png"></p>

<p>SNS通知</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Service: AWS Auto Scaling
</span><span class='line'>Time: 2013-12-14T14:39:41.271Z
</span><span class='line'>RequestId: f622c6d2-8c77-4fef-8b38-ece463574712
</span><span class='line'>Event: autoscaling:EC2_INSTANCE_TERMINATE
</span><span class='line'>AccountId: 111155559999
</span><span class='line'>AutoScalingGroupName: test-sg
</span><span class='line'>AutoScalingGroupARN: arn:aws:autoscaling:ap-northeast-1:11115559999:autoScalingGroup:0e771015-f979-4afe-b065-595abafdbf9b:autoScalingGroupName/test-sg
</span><span class='line'>ActivityId: f622c6d2-8c77-4fef-8b38-ece463574712
</span><span class='line'>Description: Terminating EC2 instance: i-b03788b5
</span><span class='line'>Cause: At 2013-12-14T14:38:38Z an instance was taken out of service in response to a system health-check.
</span><span class='line'>StartTime: 2013-12-14T14:38:38.257Z
</span><span class='line'>EndTime: 2013-12-14T14:39:41.271Z
</span><span class='line'>StatusCode: InProgress
</span><span class='line'>StatusMessage:
</span><span class='line'>Progress: 50
</span><span class='line'>EC2InstanceId: i-b03788b5</span></code></pre></td></tr></table></div></figure>


<p>やはり20分のタイムラグ変わらずですね。。</p>

<h3>結論</h3>

<p>というわけで、EC2 Status Checkが1分間隔になっても、EC2のみ（ELBを使わず）のAuto Scalingによる自動復旧時間は変わらずでした。</p>

<p>ちなみにAWS ConsoleでAuto Scalingの設定ができるようになったけど、まだscaling groupにtagが付けられないのがちょっと微妙ですね。。GUIで状態を見る分には楽だけど。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/12/13/serverfesta-2013-autumn/">サバフェス！2013に参加してきた</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-12-13T15:59:00+09:00" pubdate data-updated="true">Dec 13<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>少し前に<a href="http://connpass.com/event/3690/">サバフェス！2013 Autumn</a>に参加してきました。
内容を忘れないうちにやった事を書いておきます。</p>

<p>スコアはトップレベルだったものの、運営側がサーバを起動した所、自動でサービスが立ち上がらなかったらしいので残念な事に参考値のみに。（提出前に2回ぐらい再起動確認したのにおかしいなぁ。。）
優勝スコアは90.830 (GET 199,802 : POST 18,106)で、私のは<strong>93.410</strong> (GET 162,923 : POST 29,930)でした。</p>

<h2>お題</h2>

<p>「最速インフラを構築せよ！！！</p>

<p>WordPressに一切手を加えずに、どこまで高速化できるのか！？</p>

<p>OSチューニング、サーバチューニング、負荷分散…最適解を探せ！」</p>

<p>&#8211;</p>

<p><a href="http://www.idcf.jp/cloud/service/self.html">IDCFクラウド</a>上で仮想マシン5台（M8タイプまで）を使ってスコアを競うというもの。
<a href="https://www.facebook.com/tuningathon">チューニンガソン</a>と似てますが、サーバが複数台使えるのが良いですね。</p>

<h2>構成</h2>

<p>表彰式でLTしたけど、LVS (DSR) + php 5.5 + apcu + Varnish + nginx (lua) + memcached</p>

<p>ポイントは</p>

<ul>
<li>GET時は1台では帯域の限界に達したのでLVS (DSR)による4台での並列応答</li>
<li>POST時には必ず各Varnishのキャッシュクリア（ban）する</li>
<li>POST時にnginx->memcachedへ渡すと高速すぎたのであえてsleepを5msして遅延させる</li>
<li>nginx(lua)はコード量が多いと通らないのでぎりぎりまで削減</li>
<li>memcachedからmysqlへの非同期処理（5ms間隔）</li>
<li>mysqlの更新処理はそんなにいらないので基本チューニングとInnoDBにしただけで、5.1のまま</li>
<li>サーバはM8までいらないのでM4で</li>
<li>海外から参戦したけど、GUIが重いのでAPI経由での操作</li>
</ul>


<p>でしょうか。特に意識したのはmemcachedからmysqlへの許容範囲内での同期と複数台あるVarnishのキャッシュクリアですね。他のチームはnginxでTTLを設定して逃げたようですが、実運用時にはPOST時にキャッシュクリアを確実にする必要があるのでVarnish moduleをコンパイルして他のsiblingへ並列でban処理を投げてました。まあ、今回のベンチマークツールはそこまで厳格じゃなかったけど、最終チェックは人間が動作させるので。</p>

<p>LT資料。スコア推移と簡単な構成の紹介</p>

<div class="embed-ss-container"><iframe src="http://www.slideshare.net/slideshow/embed_code/29171459 "></iframe></div>


<h2>設定ファイル</h2>

<p>以下、設定ファイルです。いらない所は削ったけど動くはず。。</p>

<p>backend varnish vcl （各backendの設定は微妙に違う）:</p>

<div><script src='https://gist.github.com/7941009.js?file='></script>
<noscript><pre><code>&lt;html&gt;&lt;body&gt;You are being &lt;a href=&quot;https://github.com/gist/7941009&quot;&gt;redirected&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;</code></pre></noscript></div>


<p>backend nginx.conf:</p>

<div><script src='https://gist.github.com/7940999.js?file='></script>
<noscript><pre><code>&lt;html&gt;&lt;body&gt;You are being &lt;a href=&quot;https://github.com/gist/7940999&quot;&gt;redirected&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;</code></pre></noscript></div>


<p>base nginx.conf:</p>

<div><script src='https://gist.github.com/7941214.js?file='></script>
<noscript><pre><code>&lt;html&gt;&lt;body&gt;You are being &lt;a href=&quot;https://github.com/gist/7941214&quot;&gt;redirected&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;</code></pre></noscript></div>


<p>base syncer.rb:</p>

<div><script src='https://gist.github.com/7941042.js?file='></script>
<noscript><pre><code>&lt;html&gt;&lt;body&gt;You are being &lt;a href=&quot;https://github.com/gist/7941042&quot;&gt;redirected&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;</code></pre></noscript></div>


<p>base supervisord.conf:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[program:syncer]
</span><span class='line'>command=/home/mho/.rvm/bin/ruby /home/mho/syncer/sync.rb 5
</span><span class='line'>stdout_logfile_maxbytes=1MB
</span><span class='line'>stderr_logfile_maxbytes=1MB
</span><span class='line'>stdout_logfile=/tmp/%(program_name)s-stdout.log
</span><span class='line'>stderr_logfile=/tmp/%(program_name)s-stderr.log
</span><span class='line'>user=mho
</span><span class='line'>directory=/home/mho/syncer
</span><span class='line'>autostart=true
</span><span class='line'>autorestart=true</span></code></pre></td></tr></table></div></figure>


<p>base my.cnf:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[mysqld]
</span><span class='line'>datadir=/var/lib/mysql
</span><span class='line'>socket=/var/lib/mysql/mysql.sock
</span><span class='line'>user=mysql
</span><span class='line'># Disabling symbolic-links is recommended to prevent assorted security risks
</span><span class='line'>symbolic-links=0
</span><span class='line'>character-set-server = utf8
</span><span class='line'>max_connections = 1000
</span><span class='line'> 
</span><span class='line'>key_buffer_size = 32M
</span><span class='line'>max_allowed_packet = 16M
</span><span class='line'>thread_stack = 192K
</span><span class='line'>thread_cache_size  = 200
</span><span class='line'> 
</span><span class='line'>#slow_query_log=1
</span><span class='line'>#long_query_time=0
</span><span class='line'>query_cache_type = 0
</span><span class='line'>skip-innodb_doublewrite
</span><span class='line'>innodb_buffer_pool_size = 192M
</span><span class='line'>innodb_log_buffer_size = 4M
</span><span class='line'>innodb_flush_log_at_trx_commit = 0
</span><span class='line'>innodb_support_xa = 0</span></code></pre></td></tr></table></div></figure>


<p>sysctl.conf:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>fs.file-max = 1048576
</span><span class='line'>
</span><span class='line'>net.ipv4.ip_local_port_range = 1024 65535
</span><span class='line'>
</span><span class='line'>net.core.wmem_max = 16777216
</span><span class='line'>net.core.rmem_max = 16777216
</span><span class='line'>
</span><span class='line'>net.ipv4.tcp_wmem = 4096 65536 16777216
</span><span class='line'>net.ipv4.tcp_rmem = 4096 87380 16777216
</span><span class='line'>
</span><span class='line'>net.core.somaxconn = 8192
</span><span class='line'>
</span><span class='line'>net.core.netdev_max_backlog = 8000
</span><span class='line'>
</span><span class='line'>net.ipv4.tcp_max_syn_backlog = 8192
</span><span class='line'>
</span><span class='line'>net.ipv4.tcp_synack_retries = 3
</span><span class='line'>net.ipv4.tcp_retries2 = 5
</span><span class='line'>
</span><span class='line'>net.ipv4.tcp_keepalive_time = 900
</span><span class='line'>
</span><span class='line'>net.ipv4.tcp_keepalive_probes = 3
</span><span class='line'>
</span><span class='line'>net.ipv4.tcp_keepalive_intvl = 15
</span><span class='line'>
</span><span class='line'>net.nf_conntrack_max = 1000000</span></code></pre></td></tr></table></div></figure>


<h2>終わりに</h2>

<p>最初はecho選手権になってたのであんまりやる気がなかったけど、後半はちょっと楽しめました。結果は惜しかったけど、今まで<a href="https://www.facebook.com/tuningathon">チューニンガソン</a>や<a href="http://isucon.net">ISUCON</a>に出場したり、トラブル☆しゅーたーずを主催したり、日々の運用等の経験が生きた感じがします。運営の皆様、ありがとうございました。次があれば楽しみにしています！（レギュレーションの解釈が微妙だったのでそこはブラッシュアップして欲しいですね）</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/11/11/isucon3-final/">ISUCON3の本戦に参加してきた</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-11-11T11:27:00+09:00" pubdate data-updated="true">Nov 11<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>先月の<a href="http://ijin.github.io/blog/2013/10/07/isucon3-preliminary/">予選に通過したの</a>で、<a href="http://isucon.net/">ISUCON3</a>の本戦に参加してきました。</p>

<p>完敗。</p>

<h3>お題発表</h3>

<p>画像版twitter。投稿する画像の公開レベルをpublic, private, followers onlyに設定できるシステムが1台のVPS（2コア、4GB RAM）で動いている状態。プレスを打った為、大量アクセスがくる18時までに別途用意された4台のサーバを使ってスケールせよという使命を与えられる。</p>

<h3>流れ</h3>

<p>画像データが1万点・約3GBあったので、まず失敗しても戻れるようにバックアップ取得を開始。それと平行して他のサーバへのsshキー登録したり、hosts書いたり、もろもろ下準備。</p>

<p>デフォルトのperlのスコアは<strong>1206.8</strong></p>

<p>言語はrubyと決めていたので、supervisorで立ち上げてみるが起動失敗。よくよく調べてみるとforemanが入ってなくてGemfileに追加してbundle。</p>

<p>この時のスコアが<strong>1180.8</strong></p>

<p>次にデータベースを見てみるものの、レコード数も比較的少なく、総容量が2MBもないのでできる事は限定されていると判断。クエリーをさらっと見た後にentriesのimageカラムに対してインデックスを張ったぐらい。</p>

<p>アクセスログにレスポンスタイムを出力するようにして1回ベンチを走らせログを解析。</p>

<p>ブラウザ上の挙動を確認しつつ、ソースコードを読んで結局画像変換のconvert処理が一番重そうだったのでそこから着手することに。</p>

<p>予選の時も外部プログラムを呼んでいるところが改善ポイントの一つだったので、まずfastimage_resizeを使って置き換えてみるものの、処理速度はそんなに上がらず、スコアもほぼ横ばい。</p>

<p>その間に、ロングポーリングの処理を変更してみるけど、</p>

<pre><code> "message": "2013-11-09T14:48:17 [36898] [CRITICAL] timeline refrection timeout"
</code></pre>

<p>タイムラインの反映がうまくいってない模様。
（ちなみにエラーメッセージのrefrectionはreflectionのスペルミスですね）</p>

<p>次に画像変換処理の部分で毎回リクエストがくる度に実行されるリンクをredisにてキャッシュ。これは効果があり、スコアは<strong>6634.2</strong>で暫定3位。</p>

<p>その間にVarnishやHaproxy + nfsを軽ーく試してみるものの、スコアは伸びず。</p>

<p>この辺でリンクだけではなく、画像自体をredisに突っ込んで全サーバで処理するアーキテクチャを決定。<a href="https://twitter.com/acidlemon">@acidlemon</a>さんと似た<a href="http://beatsync.net/main/log20131110.html">構成</a>ですね。ただ違うのはPOST後のsidekiqを使って処理を裏のワーカーに任せるという事。</p>

<p>sidekiqが動作するところまではでき、全画像の変換を試みるがredisサーバのメモリが溢れてたので、最初にアクセスされる直近30件、アクセス比率が高いサイズsと、新規画像のみに注力。スコアは徐々に上がる。</p>

<p>その後はただひたすらに、もくもく実装とデバッグ。</p>

<p>残り3-40分ぐらいのところで、生ハムチームでブレークスルーが起こり、彼らが一気にトップへ踊り出る。我々も1台構成であれば5位ぐらいにはなれただろうけど、スケールアウトしなければ全く勝負にならないので最後の最後まで果敢に挑戦するもあえなくタイムアップ。</p>

<p>結果、FAIL。</p>

<h3>感想</h3>

<p>今回はサーバが5台もあったので、スケールアウトしなければならないのは明白で、実装を真っ先に着手するべきでしたね。前半で1台だけチューニングして後でスケールしようと思ったのが戦略上の致命的ミス。時間切れで終わったので実装が間に合っていたらそれなりのスコアが出たはずかと。優勝した生ハムチームが結構ギリギリまでかかったのを考えると、やはり見極めたポイントは重要で、さすがとしか言いようがないです。また、一番時間のかかった画像配信に関しては普段AWSを使っている身としてはs3へ画像を突っ込むのが当然だと考えていたので、なかなか新鮮で違う脳を使う感じで楽しめました。</p>

<h3>その他</h3>

<ul>
<li>途中ディスカッションをすれば良かった（予選は上々でも本戦で焦ってしまった）</li>
<li>落ち着いて俯瞰して見るべし。見極め大事</li>
<li>予選とかの先入観が邪魔したのでまっさらの状態で考えるべき</li>
<li>ベンチマークツールはFAILしても再実行に2分待たされるのがどうしてももどかしかった</li>
<li>ベンチマークツールの他のサーバへの実行切替がバグってて時間をロスった</li>
<li>チーム名のRevengeが果たせなかったので、来年はチーム名どうしようかな。。</li>
<li>ドヤモリスが満面の笑みで幸せそうだった</li>
</ul>


<p>ISUCONのレベルも毎回毎回レベルが上がっていき、運営側の苦労が伺えます。本当にお疲れ様でした！また来年にも期待しています！</p>

<h3>おまけ</h3>

<p>さて。1年間待ち望んだイベントがあっという間に終わってしまって消失感・焦燥感を味わいつつも、気を取り直して次はAWS re:InventのGAMEDAYに日本から唯一（多分）の参加者として参戦します！</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/10/07/isucon3-preliminary/">ISUCON3の予選を通過した（はず）</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-10-07T12:12:00+09:00" pubdate data-updated="true">Oct 7<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://ijin.github.io/blog/2012/11/05/isucon2/">去年</a>に引き続き参加した<a href="http://isucon.net/">ISUCON</a>第3回目の予選を通過しました。</p>

<p><a href="http://isucon.net/archives/32848462.html">結果</a>は参戦1日目の方で<strong>4位</strong>。
74チーム中、総合<strong>6位</strong>になりました。</p>

<h3>事前準備</h3>

<p>再び<a href="https://twitter.com/cads">@cads</a>と<a href="https://twitter.com/fruwe">@fruwe</a>に声をかけ、去年3位という雪辱を果たすべく、「The Revenge of Mr. Frank &amp; Co.」とチーム名を改名。
今年はチーム数が膨れると予想されてたので去年と違って予選がありトップ20位ぐらいまでしか本戦に行けないという仕組み。方針は以下の通り。</p>

<ul>
<li>言語は我々の得意なruby（goも面白そうだったけど）で勝負</li>
<li>去年Railsの再実装で失敗した為、Sinatra（であれば）でやりきる</li>
<li>Sinatra、Varnish、Sidekiq、Redisの復習</li>
<li>計測ツールの導入</li>
<li>早い時期に全体のスケジュールを決定する</li>
<li>戦略転換の見極め</li>
</ul>


<p>特に去年の敗因が戦略転換の遅れだったので、それを意識するようにしました。</p>

<h3>お題発表</h3>

<p>Markdownを使ったコメントシステム。しかもユーザログインがあってポストのprivate/public指定ができるというちょっとアドバンストな作り。</p>

<h3>前半戦</h3>

<p>今回、サーバはAWSのAMIとして提供され自分のアカウントで起動するやり方だったので、<a href="http://d.conma.me/entry/2013/04/08/190229">CPUガチャ</a>に若干期待しつつ10台一気にlaunchする。確認したら全部同じCPUだったので半分落として、後は本番用、開発用、バックアップ用として残す。</p>

<p>まずは、状況把握の為にひとまず皆でルールの理解とソース解析。ざっと目を通した後はブラウザで動作確認し、今回同一マシン内で実行させるベンチマークプログラムを複数言語で実行。制御方法は去年と同じsupervisord</p>

<p>Perl</p>

<pre><code>2013/10/05 10:31:15 done benchmark
Result:   SUCCESS
RawScore: 1020.4
Fails:    0
Score:    1020.4
</code></pre>

<p>Ruby</p>

<pre><code>2013/10/05 10:38:11 done benchmark
Result:   SUCCESS
RawScore: 2446.9
Fails:    0
Score:    2446.9
</code></pre>

<p>Go</p>

<pre><code>2013/10/05 10:45:32 done benchmark
Result:   SUCCESS
RawScore: 2840.1
Fails:    0
Score:    2840.1
</code></pre>

<p>Node</p>

<pre><code>2013/10/05 10:49:07 done benchmark
Result:   SUCCESS
RawScore: 1543.4
Fails:    0
Score:    1543.4
</code></pre>

<p>お、rubyそんなに悪くないかも。
デフォルトのperlが遅いのはメモリが温まってないかなと思って再実行したけど飛躍的に良くはならなかったですね。</p>

<p>この辺で予め決めていた開始後1時間という期限になったので作戦会議とスケジュール策定に入る。</p>

<p>まずはVarnishを入れてみようという事なので一人がその作業にとりかかる。もう一人はNewrelicという測定ツールの導入とバックアッププランのRedisバージョンの作りこみ。私はDB周りのチューニングやミドルウェア周り、後は二人のサポート。</p>

<p>DBはMySQLだったので全クエリを吐き出してpt-query-digestで解析して統計的に遅いクエリ順に潰す。
schemaをよく見たらindexがなかったので張ったり、クエリーをちょっと改善してみたり、パラメータを変更（innodb_buffer_pool_sizeとinnodb_flush_log_at_trx_commitのみ）したり黙々作業。</p>

<p>時間はあっという間に過ぎて両方のバージョンを導入してみるもののベンチマークツールからFAILを大量にくらい、スコアが全然つかず。。。</p>

<p>折り返し時点が迫ってきたので、とりあえず今あるチューニングだけのスコアを送信。</p>

<pre><code>2013/10/05 13:43:37 done benchmark
Result:   SUCCESS
RawScore: 3791.5
Fails:    6
Score:    3450.2
</code></pre>

<h2>後半戦</h2>

<p>その後、unicornのワーカー数を調整したり、クエリーを更に見なおしたり、initializer作って事前にキャッシュを温める仕組みを作ったりして6000ぐらいのスコアを出した気がする（この辺は作業ログを残すのを忘れていたので記憶が曖昧）</p>

<p>Redisバージョンは雰囲気的に無理そうだったので早々に捨ててこの辺は去年の教訓が生かされてると思う）ESI対応 + Varnishに注力する事を決断。しかし、こっちもエラーの連発。。</p>

<p>最も悩まされたのは</p>

<pre><code>14:53:20 [FAIL] invalid Cache-Control header
14:53:21 [FAIL] invalid post memo URL http://localhost/
</code></pre>

<p>どうやらリバースプロクシ用に付与しているヘッダーチェックで弾かれていた模様。</p>

<p>VCLファイルをよく見たら既存のシステムから流用したモノらしく、余計なルールがたくさん入っていたので、仕切り直しということで一旦白紙から再作成する事に。</p>

<p>なんとか基本部分だけ動かす事ができるようになってスコア送信。</p>

<pre><code>2013/10/05 15:45:18 done benchmark
Result:   SUCCESS
RawScore: 14694.2
Fails:    0
Score:    14694.2
</code></pre>

<p>この辺で暫定一位。</p>

<div class='embed tweet'><blockquote class="twitter-tweet"><p><a href="https://twitter.com/search?q=%23isucon&amp;src=hash">#isucon</a> オンライン予選一日目 中間発表二回目です。残り2時間となった16時時点での順位は以下の通り&#10;1. The Revenge of Mr. Frank &amp; Co.&#10;2. hidekiy&#10;3. ( (0) / (0)) ☆祝☆&#10;4. パイの実g&#10;5. 白金動物園&#10;続く</p>&mdash; 941 (@941) <a href="https://twitter.com/941/statuses/386386593696604161">October 5, 2013</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></div>


<div class='embed tweet'><blockquote class="twitter-tweet"><p>isuconは外国県人会が暴れてるのかな？</p>&mdash; ばば としあき (@netmarkjp) <a href="https://twitter.com/netmarkjp/statuses/386393051230261248">October 5, 2013</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></div>




<br />


<p>しかしその後privateページのキャッシュexpire方法を模索するも、なかなか進まずにタイムリミットが刻々と迫り、他のチームがどんどん上位に食い込んできて若干焦り始める。ひとまず、強制フラッシュするエンドポイントと裏で定期実行するバックグランドタスク（Varnishでいう所のbanlurker相当）を即席で実装したりしてスコアを伸ばすもトップには届かず敢え無く終了。</p>

<p>振り返ってタイムスタンプを見るとギリギリまで粘っていたのが分かりますね。</p>

<pre><code>-rw-r--r-- 1 root root 2322 Oct  5 17:59 /etc/supervisord.conf
</code></pre>

<p>結果、送信できた最終スコアは「<strong>20599.5</strong>」でした（ローカルスコアは22000ぐらいだったけど）。
予選突破（暫定）はできたものの、まだまだ課題山積な感じです。</p>

<h3>良かった点</h3>

<ul>
<li>最初に戦略とスケジュールを策定できた</li>
<li>去年学んだ捨てる勇気を持てたこと</li>
<li>去年よりいろいろ試せたので敗北感は改善</li>
<li>オンライン参戦なので普段使い慣れてる環境で落ち着いて出来た（本戦はアウェー）</li>
<li>今年の密かな目標であるモリスさんに勝ったこと（予選だけど）</li>
</ul>


<h3>反省点</h3>

<ul>
<li>作業ログをもっとしっかり取るべき</li>
<li>バージョン違いはgit mergeせずにbranchをcheckoutするべき</li>
<li>lingrサポート見ればよかった</li>
<li>MySQL5.6のMemcache APIの存在には気づいてたけど、罠とは知らなかった</li>
<li>Newrelic導入に時間かけた割には得られる情報量が薄かった</li>
<li>リバースプロクシが遅れたのは全実装して投入を試みたからでincremental apporachの方が良かった</li>
<li>全員のタスクマネジメントとペアプログラミングをもっとすれば良かった</li>
</ul>


<p>今回も前回同様、非常に楽しめました。
運営の皆様、お疲れ様でした&amp;本戦はさらなる期待をしてます！</p>

<h3>おまけ</h3>

<p>後日行われた各チームの反省会で判明したのがworkloadの存在。どうやらベンチマークツールの並列度を上げる事ができるらしく、そこそこ速くなっているシステムだったらスコアが1.5倍ぐらいは伸びたかもとの事。最終スコアを考えると、30000点台のトップを取れたかもしれないのが無念。。アプローチ自体は総合トップの<a href="https://twitter.com/sechiro">@sechiro</a>さんと<a href="http://sechiro.hatenablog.com/entry/2013/10/07/%23isucon_2013_%E4%BA%88%E9%81%B8%E3%82%92%E3%83%88%E3%83%83%E3%83%97%E9%80%9A%E9%81%8E%E3%81%97%E3%81%A6%E3%81%8D%E3%81%9F%EF%BC%88%E3%81%AF%E3%81%9A%EF%BC%89%E3%80%82">似ていた</a>ので方針は間違ってなかったかと。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/06/10/aws-game-day-tokyo-2013/">AWS Game Day Tokyo 2013で受賞してきた</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-10T23:33:00+09:00" pubdate data-updated="true">Jun 10<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>大統領選挙でオバマ陣営のシステムを堅牢化する為に用いた手法である仮想対戦シミュレーション「<a href="http://gameday2013.doorkeeper.jp/events/3960">AWS Game Day Tokyo</a>」が日本で初めて（世界2番目に）開催されたので参加してきました。</p>

<p>結果、ベスト・ディフェンス賞こと「<strong>Most Awesome Fix!</strong>」賞を受賞しました。</p>

<h2>経緯</h2>

<p>以前、<a href="http://jaws-ug.jp/jawsdays2013/">JAWS DAYS 2013</a>でMiles Wardが講演した「<a href="http://www.publickey1.jp/blog/13/obama_for_america.html">Behind the Scenes of the Presidential Campaign</a>」でチームを攻撃・防御に分けて対戦させ、そこから学んだ事をフィードバックしてシステムをより堅牢にするという「Game Day」を知り、日本でもやりたいねという話になってました。そこで先日の<a href="http://www.awssummittokyo.com/">AWS Summit Tokyo</a>のスピンオフイベントとして、Milesの再来日に合せてADSJ（アマゾンデータサービスジャパン株式会社）さんによって開催される事になりました。チーム戦の大会は<a href="http://ijin.github.io/blog/2012/07/03/tuningathon4/">チューニンガソン</a>や<a href="http://ijin.github.io/blog/2012/11/05/isucon2/">ISUCON</a>以来なので、わくわくしながら速攻で応募をしました。</p>

<h2>概要</h2>

<p>大体、こんな流れです。</p>

<ul>
<li>システムの構築・堅牢化</li>
<li>相手システムの攻撃（この間、自システムも攻撃される）</li>
<li>自システムの修復</li>
<li>評価</li>
</ul>


<p>それぞれ、2〜3人のチームに別れ、計18チームにより対戦。私のチーム名は時事ネタとして今流行りの「<a href="http://jp.techcrunch.com/2013/06/07/20130606report-nsa-collects-data-directly-from-servers-of-google-apple-microsoft-facebook-and-more/">PRISM</a>」としました。
システム概要はnginksさんの<a href="http://d.hatena.ne.jp/nginks/20130608/1370700185">ブログ</a>が非常に分かりやすいです。
要するに画像変換処理バッチクラスターですね。</p>

<h2>構築</h2>

<p>手順書をざっと見ながら画像処理クラスターを構築。相方はシステムの人間ではなかったので、動作確認を手伝ってもらいつつ実質一人でもくもくと作業。s3作成、sqs作成、アプリのインストール・設定・動作確認、AMI化、Cloudwatch設定、AutoScaling設定等を淡々と。構築しながらシステムを把握して行くけど、結構これだけで時間がとられます。なので、じっくりと防衛策は練れなかったのでひとまず、プロセスの自動復旧をしてくれるmonitをインストール・設定し（upstartでやりかけたけどうまく動かなかった）、主要ファイルのchecksumを取って改善検知してメール通知する仕組みを導入。</p>

<h2>攻撃</h2>

<p>AWSキー（Poweruser権限）を奪取したという仮定の元、相手システムに攻撃をしかけるターン。
単純に全部消したり、セキュリティグループの権限を変えたりではあまりにもつまらないので、いろいろ考えます。（無論、システムの全消しは誰でもできる最低の攻撃手法）</p>

<p>まず、状況把握する為にいろいろ動作確認。キューに画像を突っ込んで、ちゃんと処理されるとか。あれ、でも動かない。。
どうやらTokyoで作りかけたけど、結局Virginiaリージョンで仕上げたと運営側から伝えられる。いきなりのタイムロス！</p>

<p>気を取り直して、稼働中のインスタンスに入る方法を思いつく。通常はキー設定されているのでsshでは入れないので仮のインスタンスを起動し旧インスタンスのroot volumeのEBSを強制detachし、仮インスタンスにattachして中身をいじってからre-attachする事に。見た目は同じinstance-idなのに中身だけ違う、一見すると分かりづらいです。そこで旧インスタンスを一旦停止させる為にstopさせると、、terminateされちゃいました。。よくよく調べて見ると、Auto Scalingの設定になっていて、min-sizeの制約によって旧インスタンスが消され、代替インスタンスが自動的に起動するようになってました。</p>

<p>どうやら構築が間に合わなかったチーム用に運営側が用意した自動構築をしてくれる虎の子のCloudformationを使った模様。
そこで、相手チームのスキルレベルがそれ程高くないと判断し、Auto Scalingの元AMIを置き換える事に。
新たなlaunch configを作成し、既存のscaling groupと同盟のものを作成。</p>

<p>次にs3への攻撃。bucket名はglobalなnamespaceなので、こいつを削除して同名のを別AWSアカウントで作れば乗っ取りが可能。。
のはずが、削除してから一定時間経過しないと作成不可だったので1字違いのbucketを作成しておく。</p>

<p>最後にs3のbucket一覧を取得して、常に空のディレクトリと同期し続ける攻撃を思いつき、実装を始める。システムはs3に出力するのでそこを継続的に空にする攻撃です。しかし、実装を初めて動作確認の途中で時間切れになりシステムに埋め込めなくて断念。もうちょっと時間が欲しかったです</p>

<h2>修復</h2>

<p>次は自システムが受けた攻撃を修復するターン。</p>

<p>いろいろ余計なインスタンスが起動していたが、まずやったのがAMI番号の確認。（これが無事であればOSに侵入されていようがAMIをベースに全体の再構築が速やかにできるので）
幸い、控えていたのと一致していたので他のインスタンスを全て停止。一応monitのアラートメールが飛んでいなかったので、インスタンスに対しての操作は限定されているのだろうとは踏んでましたが。</p>

<p>AMIが無事なら次はAuto Scalingの確認。ざっと見た感じだと、lauch configは操作されておらず、scaling groupのmin-sizeが0に変更されている模様。他の変更点は確認が面倒だったので、一旦全部削除してさくっと再作成。後で聞いた話だと、Auto Scalingのrecurring schedule設定で1分起きに0台にする設定をしていたらしいが、消された時点で攻撃は無効化。</p>

<p>次にSQS。消して再作成すればてっとり早いけど、相手チームがキューに投入した画像を最終的に表示させないといけないルールだと誤解していて、その復旧に務める。新しく作ったSQSと比較するとパラメータ（Default Visibility Timeout, Retention Period, Message Size等）が異常な値に変更されていると分かり、通常の値へ戻す。</p>

<p>この時点でアプリとSQSの通信を確認するも疎通できない事を把握。pingが通らない事からSecurity Group, Routing Table, Network ACLが変更されていないかを確認。どうやらSecurity GroupのIn/Outルールが削除されている単純な攻撃だと判明し、なんなく再設定。</p>

<p>キュー内のメッセージが1コ処理されるのを確認し、SQS周りは対応済みかと思ったけど残り2コのメッセージがいくら待てども処理されず、若干悩む。
ログを見たり、メッセージの中身を覗くとと「<strong>&#8211;max-redirect=99999999</strong> 」が目に留まる。どうやら変換する画像をダウンロードする部分で無限ループさせている模様。メッセージを削除し、そのパラメータを除外したものを流してちゃんとキューが処理される事を確認。</p>

<p>最後にs3周りで怪しい設定がないかを調査して、一通りの動作確認をして復旧完了。</p>

<h2>振り返り</h2>

<p>お互いに対戦したチームと顔合わせをし、攻撃や復旧の手の内を明かします。全チームの行動記録を集約して運営側で審査を行われ各賞が授与される中、我がPRISMはSQS内の無限ループを検知・修復したのが評価されて最も優れた修復を行った「<strong>Most Awesome Fix!賞</strong>」を頂きました。後で他のチームに聞いた所、monitのような検知・通知の仕組みを導入した所はなさそうだったので、それも評価ポイントだったかも知れません。</p>

<p>賞の内容としては、ワンタイムトークンを生成するハードウェアMFAデバイスとAWSのクーポンコードでした。ありがとうございます。</p>

<p>最後にMilesが壊れても戻せるようにあるべき状態の定義と常に比較して自動的に自己治癒するのが最高のシステムと言ってました。AWSの状態を保存するにはCloudformerでCloudformationテンプレート化すれば便利で楽だけど、chefみたいにIdempotency（冪等性）を継続的に保証する仕組みをそのレイヤーで組むのはなかなか大変ですね。（個々のサーバ単位は可能だとしても）</p>

<h2>感想</h2>

<p>「<em>チューニンガソン</em>」や私が他にお手伝いをしている「<em>トラブル☆しゅーたーず</em>」とも一味違って、非常に楽しめました。</p>

<p>以下、思った事をいくつか。</p>

<ul>
<li>構築に時間が取られたのでなるべく出来合いのシステムがあった方が防衛策に専念できそう</li>
<li>攻撃可能な時間が思ったより短かったのでもうちょっと長めで</li>
<li>Default-VPCとEC2-Classicでは挙動が違うのでアカウントタイプは統一した方が良い</li>
<li>ターン性ではなく、攻撃と修復・防御のリアルタイム性を試すとか</li>
<li>あるべき正しい状態を把握する為のツールがあると他のタスクに集中できるかも</li>
</ul>


<p>次に開催される時も参加したいですね。もしくは運営側のお手伝いでも！</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/05/21/custom-non-rds-multi-az-mysql-replication/">Non-RDSなオレオレMulti-AZ MySQL Replication</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-21T18:03:00+09:00" pubdate data-updated="true">May 21<span>st</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>先日（5/17/13）<a href="http://www.zusaar.com/event/668006">cloudpack night #6 - Re:Generate -</a>に参加してきました。</p>

<p>当日の様子はcloudpack<a href="https://twitter.com/yoshidashingo">吉田</a>さんの以下のレポートで。<br />
<a href="http://d.hatena.ne.jp/yoshidashingo/20130518/1368853720">cloudpack Night #6 - re:Generate - を開催しました</a></p>

<p>私はDJを少々とLTで参加させて頂いたのでその内容になります。</p>

<h2>オレオレMulti-AZのススメ</h2>

<p>AWS上でMySQLを使う場合、RDSはてっとり早くて良いんですが、たまにもうちょっと柔軟性が欲しい時があります。
例えば別のストーレジエンジンやディストリビューションを使ったり（Percona Server, MariaDB, TokuDB, Mronnga等）、RDSでは使えないインスタンスファミリー（hi1, m3, c1系）を使ったり、OSレベルでのチューニングができたり、スレーブのバッファプールを予め温めておいたり。等々。</p>

<p>しかし、RDSにはAvailability Zone (AZ)をまたいでフェールオーバーするMulti-AZ機能があり、AWSで設計するにはAZ障害を考慮した方が推奨されます。</p>

<p>そこで、MHAとVPCを組み合わせて柔軟性をもったMulti-AZ環境を実現します。
（ちなみに私の場合はhi1.4xlargeでPercona Serverを冗長化をする必要があったから）</p>

<h3>MHA</h3>

<p><a href="https://code.google.com/p/mysql-master-ha/">MHA</a>とはFacebookの<a href="http://yoshinorimatsunobu.blogspot.jp/">松信</a>さんがDeNA時代に作ったMySQLの自動フェールオーバーしてくれうナイスなツールです。Master障害時にbinlog同期とSlaveの昇格を全自動でやってくれます。昇格時にはカタログデータベースに更新をかけて新DB構成のIPの情報を更新するか、Virtual IPを切り替えるかをする必要（この担当部分はmaster_ip_failover_scriptで対応）があるけど、今回は後者的なアプローチになります。</p>

<h3>VPC</h3>

<p>ENIを使えばVirtual IP的な使い方はできるけどAZは超えられないので、source/dest. checkを無効化した上でrouting tableによって擬似Virtual IPを実現する<a href="http://aws.clouddesignpattern.org/index.php/CDP:Routing-Based_HA%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3">Routing-Based HAパターン</a>（CDP 2.0候補）を使います。ADSJ<a href="https://twitter.com/c9katayama">片山</a>さんの<a href="http://d.hatena.ne.jp/c9katayama/20111225/1324837509">エントリ</a>が発端ですね。</p>

<h2>Demo</h2>

<p>以下、LT時に見せたデモ動画</p>

<div class="embed-video-container"><iframe src="http://www.youtube.com/embed/tovb29K6ddc "></iframe></div>


<p>擬似Virtual IPに対してそれぞれread/writeを行いつつMasterのプロセスをkillすると、通信できなくエラーが出続けるが20秒以内に自動フェールオーバーが完了しread/write共に再開します。pingも平均0.5msから2.0msに変わった事からAZが移った事が確認できます。</p>

<p>RDSのMulti-AZの場合、フェールオーバーには3-6分かかるので、かなり短時間で復旧が可能です。
また、RDSはCNAME切替によるDNSベースに対して、MHA+VPC構成の場合はIP指定することができます。そうするとアプリからのresolveが不要になり、去年起こった内部DNSが引けない障害が起きても問題ありません。</p>

<h2>注意点</h2>

<ul>
<li>RDSな自動バックアップがない

<ul>
<li>Xtrabackupとbinlogの定期s3保存で対応</li>
</ul>
</li>
<li>Point in Timeリカバリー

<ul>
<li>Chef等で自動化しましょう</li>
</ul>
</li>
<li>Read Replicaの作成

<ul>
<li>Chefで頑張りましょう</li>
</ul>
</li>
<li>学習曲線

<ul>
<li>勉強しましょう</li>
</ul>
</li>
<li>API backplaneがSPoF

<ul>
<li>AWSに祈りましょう</li>
</ul>
</li>
</ul>


<p>特に一番の懸念点は最後のAPI backplane。AWSの今までの大規模障害状況を見ていると、皆が同時に復旧をしようとしAPIへのリクエストが大量に集中してそこがボトルネックになり、リソースの操作不能に陥るという悲惨な事象が何回かありました。まあ、その場合はRDSでも同じような気はしますが、ここは当時の障害を経験にキャパシティが増加されている事を信じておくしかありませんね。。</p>

<h2>おわりに</h2>

<p>とまあ、こんなLTをしたわけですが、この後に続いたCookpadの<a href="https://twitter.com/sgwr_dts">菅原</a>さんの<a href="http://www.slideshare.net/winebarrel/ec2keepalivedlvsdsr">LT</a>の方が盛り上がって自分のは余興に終わってしまいました。</p>

<p>あ、ついでにその日はcpniteの資料作成やDJの選曲であんまり寝てなかったにもかかわらず、無事AWSソリューションアーキテクト（Associate）の認定試験に受かりました！</p>

<p><img src="https://lh5.googleusercontent.com/-d_HVsb6DgBc/UZs39XB1KGI/AAAAAAAAAuw/ntgPp33hcOI/w294-h120-no/Solutions-Architect-Associate.png"></p>

<h2>LTスライド</h2>

<div class="embed-ss-container"><iframe src="http://www.slideshare.net/slideshow/embed_code/21341276 "></iframe></div>



</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/04/03/accelerating-cross-region-data-transfer/">リージョン間高速データ転送</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-03T09:05:00+09:00" pubdate data-updated="true">Apr 3<span>rd</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>先日の<a href="http://jaws-ug.jp/jawsdays2013/">JAWS DAYS</a>のセッション<a href="http://www.publickey1.jp/blog/13/obama_for_america.html">「Behind the scenes of Presidential Campaign」</a>でリージョン間のデータ転送を高速化するツールとして<a href="http://tsunami-udp.sourceforge.net">tsunami udp</a>と<a href="http://www.cloudopt.com">cloudopt</a>を使った話が出てたので試してみました。</p>

<p>通常、遠距離のサーバはRTTが大きくなるのでスループットが下がり、巨大なデータ転送には苦労しますが、なんと27TBを9時間で転送したとのこと！実際はマシンを並列にして同時転送したらしいので1台での実験です。</p>

<h3>前提</h3>

<ul>
<li>東京(server1) -> アメリカ西海岸(server2)</li>
<li>RTT: 120msぐらい</li>
<li>EC2 instance type: m1.large</li>
<li>OS: Ubuntu 12.04</li>
</ul>


<h3>ベーステスト</h3>

<p>10Gファイルの作成</p>

<pre><code>server1$ mkdir _tmp; cd _tmp
server1$ dd if=/dev/zero of=10G count=1 bs=1 seek=10G
</code></pre>

<p>転送</p>

<pre><code>server1$ scp -rp 10G server2:
10G     100%   10GB  11.1MB/s   15:22
</code></pre>

<p>だいたい、90Mbpsぐらい。</p>

<h3>Tsunami UDP</h3>

<p>インストールは両サーバで</p>

<pre><code>sudo apt-get install make gcc autoconf
wget http://downloads.sourceforge.net/project/tsunami-udp/tsunami-udp/tsunami-v1.1-cvsbuild42/tsunami-v1.1-cvsbuild42.tar.gz
tar xvfz tsunami-v1.1-cvsbuild42.tar.gz
cd tsunami-udp-v11-b42
make
sudo make install
</code></pre>

<p>tsunami udpはfetch型の作りなので、送信側のサーバ（server1）で対象ファイルのあるディレクトリに移動してサービス起動</p>

<pre><code>server1$ cd _tmp
server1$ tsunamid *
</code></pre>

<p>で、クライアント(server2)からファイルをfetch</p>

<pre><code>server2$ tsunami connect ec2-xx-x-xx-x-x get 10G quit
</code></pre>

<p>本当はftp-likeは対話型クライアントだけど、ワンライナーでも可能。  <br/>
転送が完了するとクライアント側で情報が出力されます。</p>

<pre><code>Transfer complete. Flushing to disk and signaling server to stop...
!!!!
PC performance figure : 224947 packets dropped (if high this indicates receiving PC overload)
Transfer duration     : 419.32 seconds
Total packet data     : 183339.18 Mbit
Goodput data          : 181958.17 Mbit
File data             : 81920.00 Mbit
Throughput            : 437.23 Mbps
Goodput w/ restarts   : 433.93 Mbps
Final file rate       : 195.36 Mbps
Transfer mode         : lossless
</code></pre>

<p>おお、確かに速い！スループットもセッションでも言ってた476Mbpsに近いし。</p>

<p>サーバ側では</p>

<pre><code>Server 1 transferred 10737418241 bytes in 419.33 seconds (195.4 Mbps)
</code></pre>

<p>ファイル自身の実際の転送レートはFinal file rateである<strong>195.4Mbps</strong>。
多分、パラメータチューニングやより速いディスクを使うとスループットはさらに向上すると予想。</p>

<h3>cloudopt</h3>

<p>次は圧縮、TCP最適化、data deduplication等、様々な技術を用いてWANを高速化する<a href="http://www.cloudopt.com">cloudopt</a>の実験。こちらは有料で、ソフトウェアをインストールしてライセンスを購入（15日間のお試しあり）して使う方法とAmazon Marketplaceでセットアップ済みの課金型AMIを起動する方法がとれます。今回はてっとり早く後者で。</p>

<p>AMIはMarketplaceでCloudoptを検索し、各リージョンで1台づつ起動。</p>

<p><img src="https://lh5.googleusercontent.com/-7PWSb_8ClIo/UVuL_bzBeAI/AAAAAAAAAtc/8Lw_bDnaPVE/s665/cloudopt_marketplace_+2013-03-30+at+2.08.07+PM.png"></p>

<p>Ubuntuベースなのが良いですね。</p>

<p>まずscpから呼べるcloudcopyを使っての転送</p>

<pre><code>server1$ scp -rp -S cloudcopy 10G server2:
10G                 100%   10GB  41.5MB/s   04:07     
CloudCopy transferred 17.22 MB, saving 99.8% of bandwidth by sending 9.983 fewer GB 
</code></pre>

<p>お、速い。しかしよく見てみると17.22MBしか転送されてません。どうやらファイル自体が全てゼロ埋めなので圧縮とdeduplicationが最大限効いているみたい。</p>

<p>では、今度は実際のDBのバックアップで転送実験（容量14GB）</p>

<pre><code>server1$ scp -rp -S cloudcopy dbbackup.tar server2:
dbbackup.tar                100%   14GB  11.2MB/s   20:42   
CloudCopy transferred 6.041 GB, saving 55.5% of bandwidth by sending 7.528 fewer GB
</code></pre>

<p>スループットはほぼ一緒だけど、転送容量が削減できてます。</p>

<p>一度転送したファイルはbyte cacheされるので、次に転送する時は差分だけ送るので高速化されます。</p>

<pre><code>server2$ rm dbbackup.tar 

server1$ tar rvf dbbackup.tar file
server1$ scp -rp -S cloudcopy dbbackup.tar server2:
dbbackup.tar                100%   14GB  20.0MB/s   11:35  
CloudCopy transferred 59.57 MB, saving 99.6% of bandwidth by sending 13.511 fewer GB
</code></pre>

<h3>リージョン間レプリケーション</h3>

<p>次はMySQLのレプリケーション実験。</p>

<p>各インスタンスでのピア設定</p>

<pre><code>server1$ sudo cloudconfig peer_add server2 server2_local_ip(10.x.x.x)
server2$ sudo cloudconfig peer_add server1 server1_local_ip(10.x.x.x)
</code></pre>

<p>これでcloudoptを使ったサーバ間のトンネルが確立されます。MySQLのレプリケーションはprivate ipでの設定が必要。
一通りレプリケーションが出来き、スレーブIOを停止した状態でマスターにしばらく書き込んだのちに再開すると、binlogが転送されるのでcloudstatsコマンドで統計が見れます。</p>

<pre><code>Component - cloudoptimizer
------------------------------------------------------------
           Number of connections:                    1
              Original data size:            111.96 MB
           Transferred data size:             52.44 MB

            Bandwidth Saving:             59.51 MB (53.2%)
</code></pre>

<p><strong>53.2%</strong>の転送容量削減！</p>

<h3>結論</h3>

<p>以上を組み合わせれば、新たなCDP候補である「<strong>リージョン間レプリケーションパターン (Cross-Region Replication Pattern)</strong>」が実現できます。</p>

<ul>
<li>巨大データをリージョン間で転送するにはtsunami udpが有効そう</li>
<li>リージョン間での差分バックアップやレプリケーション向けにはcloudoptで高速化</li>
<li>普通のHTTP通信とかも使えるかも</li>
<li>s3へのアップロード高速化も対応しているのでいずれ試してみたい</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/02/22/mysql-benchmarks-on-aws-ssd-vs-fusion-io/">AWS SSD (hi1.4xlarge) vs Fusion-IOでのMySQLベンチマーク</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-02-22T17:32:00+09:00" pubdate data-updated="true">Feb 22<span>nd</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>（※ <a href="#add">追記</a>しました - 5/19/13）</p>

<p>巷ではMySQL 5.6 GAが出て騒がしいですが、ちょっと前に5.5系でAWSのSSDインスタンス（hi1.4xlarge）に載せ替える案件があったので、その時に取ったベンチマークを公表します。以前Fusion-IO (ioDrive Duo)でも同じようにやったので、比較になれば。</p>

<h2>経緯</h2>

<ul>
<li>あるウェブサービスのDBサイズが巨大でm2.4xlargeでも辛くなってきている</li>
<li>アクセスパターンによりパーティショニングが効かない</li>
<li>シャーディングをするにはアプリ改修が大変</li>
<li>数週間後に急激なアクセスが予想され、時間的余裕がない！</li>
<li>データサイズの急激な増加によりbuffer poolから溢れ、ディスクアクセスのさらなる発生が懸念</li>
</ul>


<p>というわけで、時間がないのでSSDへの移行を検討し、ベンチマークを取りました。</p>

<h2>ベンチマーク</h2>

<p>buffer poolが徐々に足りなくなった場合のディスクアクセスの発生をシミュレート</p>

<ul>
<li>sysbenchのoltpモード</li>
<li>データサイズは12G（5000万件）</li>
<li>readonly</li>
<li>uniform（フルスキャン）</li>
</ul>


<p>主要my.cnfパラメータ</p>

<pre><code> sync_binlog = 0
 innodb_buffer_pool_size = XXG
 innodb_flush_method = O_DIRECT
 innodb_flush_log_at_trx_commit = 1
 innodb_file_per_table
 innodb_io_capacity = 2000
</code></pre>

<p>コマンド</p>

<pre><code> time sysbench --test=oltp --oltp-table-size=50000000 --db-driver=mysql --mysql-user=root prepare                                                                                                         
 time sysbench --test=oltp --oltp-table-size=50000000 --db-driver=mysql --mysql-user=root --num-threads=16 --max-requests=0 --max-time=180 --init-rng=on --oltp-read-only=on --oltp-dist-type=uniform 2&gt;&amp;1 run                                                                                                         
</code></pre>

<h2>結果</h2>

<p>トランザクション推移</p>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0Aliw9SoXFJNMdFhhcHJkcDA5MGlackNHTXlPcWt0VWc&oid=2&zx=ii4lryrf8pz8"></p>

<p>レスポンスタイム推移</p>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0Aliw9SoXFJNMdFhhcHJkcDA5MGlackNHTXlPcWt0VWc&oid=3&zx=c2drap7b5561"></p>

<p>Fusion-IOと比べて半分ぐらい。ioDrvie Duoの公称IOPSが250,000+ IOPSでhi1.4xlargeが120,000 IOPSなので、まあ合致しますね。</p>

<p>まだ整理されてないベンチマーク結果があるので、後ほど追記しようと思います。</p>

<h2>追記 <a name="add">&nbsp;</a></h2>

<p>先日（5/17/13）の<a href="http://www.zusaar.com/event/668006">cloudpack night #6 - Re:Generate -</a>でADSJの荒木さんの発表でMySQLのパフォーマンスの話があったので<a href="https://code.launchpad.net/~percona-dev/perconatools/tpcc-mysql">tpcc-mysql</a>でベンチマークを取った資料を思い出し追記しました。</p>

<ul>
<li>500 warehouses (50GBぐらい)</li>
<li>24GB Buffer pool</li>
<li>16スレッド</li>
<li>1時間実行</li>
</ul>


<p>コマンド（ロードはかなり時間がかかるので注意）</p>

<pre><code> tpcc_load localhost tpcc root "" 500
 tpcc_start -d tpcc -u root -p "" -w 500 -c 16 -r 300 -l 3600
</code></pre>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0Aliw9SoXFJNMdFhhcHJkcDA5MGlackNHTXlPcWt0VWc&oid=4&zx=y6kwyezb1qth"></p>

<p>hi1.4xlargeの方が安定するまで少し時間がかかってます。<br />
以下、安定化しだした900sあたりからの数値です。</p>

<table>
<thead>
<tr>
<th></th>
<th>Fusion-IO</th>
<th>hi1.4xlarge</th>
</tr>
</thead>
<tbody>
<tr>
<td>total</td>
<td>2288758</td>
<td>1444417</td>
</tr>
<tr>
<td>avg</td>
<td>8445.6</td>
<td>5330.0</td>
</tr>
<tr>
<td>stddev</td>
<td>245.7</td>
<td>132.8</td>
</tr>
</tbody>
</table>


<p>Fusion-IOに比べて6割ってところでしょうか。
今回はSSDのephemeral disk1本で試したので、RAID0にするともうちょっと違ってくると思います。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/02/08/self-healing-with-non-elb-autoscaling/">非ELBなAutoscalingによる自動復旧</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-02-08T09:29:00+09:00" pubdate data-updated="true">Feb 8<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>バッチ処理等、サーバの冗長化が難しく仕方なく1台で動かさざるを得ない場合があります。でも可用性は確保したい。また、Pacemakerやkeepalived等できなくはないが、お金もあんまりかけられない場合もあります。
そんな時にAWS上でよく使うのがAutoscalingによる1台構成です。最低台数・最大台数共に「1」に設定しておけばEC2インスタンスが壊れても自動的に新しいのにリプレースされて復旧されます。</p>

<p>しかし、Autoscalingのhealth-check-typeを「EC2」にした場合、インスタンスの起動状態（running, stopped）しか見てくれないので、今までこの構成を実現するのにELBによる死活監視が必要でした。インスタンスがHTTPサーバじゃない場合、ちょっとムダです。</p>

<p>ところが、ちょっと前にAutoscalingがインスタンスの健康状態をチェックするEC2 status checkに<a href="http://aws.amazon.com/about-aws/whats-new/2012/12/14/auto-scaling-now-uses-amazon-ec2-status-checks/">対応</a>し、ELBが不要になったはずなので試してみました。</p>

<p>今回は各種AWSサービスに対応した統合されたPython版の<a href="http://aws.amazon.com/cli/">AWS CLI</a>ツールを使います。</p>

<h3>セットアップ</h3>

<p>まずは、ツールのインストール</p>

<pre><code>sudo apt-get install python-pip
sudo pip install awscli
complete -C aws_completer aws
</code></pre>

<h3>Autoscaling設定</h3>

<p>Launch Configの設定</p>

<pre><code>aws autoscaling create-launch-configuration --image-id ami-4a12aa4b \
--launch-configuration-name test-lc --instance-type t1.micro --key-name ijin-tokyo \
--security-groups test --iam-instance-profile test_iam

{
    "ResponseMetadata": {
        "RequestId": "c0e66974-7103-11e2-9780-a53199bac60e"
    }
}
</code></pre>

<p>Scaling Groupの設定</p>

<pre><code>aws autoscaling create-auto-scaling-group --auto-scaling-group-name test-sg \
--launch-configuration-name test-lc --min-size 1 --max-size 1 \
--health-check-grace-period 180 --tags '{"key":"Name", "value":"as-test"}' \
'{"key":"Use Case", "value":"test"}' --availability-zones ap-northeast-1a --health-check-type "EC2"

{
    "ResponseMetadata": {
        "RequestId": "e3808ef3-7103-11e2-9780-a53199bac60e"
    }
}
</code></pre>

<p>通知</p>

<pre><code>aws autoscaling put-notification-configuration --auto-scaling-group-name test-sg \
--topic-arn arn:aws:sns:ap-northeast-1:521026608000:test \
--notification-types autoscaling:EC2_INSTANCE_LAUNCH autoscaling:EC2_INSTANCE_TERMINATE \
autoscaling:EC2_INSTANCE_LAUNCH_ERROR autoscaling:EC2_INSTANCE_TERMINATE_ERROR

{
    "ResponseMetadata": {
        "RequestId": "f68359be-7103-11e2-9a1a-5f77b12b596e"
    }
}
</code></pre>

<p>レスポンスがjsonなのが良いですね。
また、<a href="http://aws.amazon.com/developertools/2535">Auto Scaling Command Line Tool</a>と違って、tagでスペースが使えるようになったのが素晴らしい！</p>

<p>以上の設定でAutoscalingによってインスタンスが1台立ち上がります。</p>

<h3>自動復旧</h3>

<p>最後にインスタンス不調（status check failure）をシミュレートする為にインスタンス内からネットワークを落とします。</p>

<pre><code>ubuntu@ip-10-128-25-25:~$ sudo ifdown eth0
</code></pre>

<p>これでstatus checkがfailし、Autoscalingが自動的に新しいインスタンスと取り替えてくれるはず！</p>

<p><img src="https://lh6.googleusercontent.com/-9FpM6ywjg84/URN6q29FD-I/AAAAAAAAAsw/paP8HBnisPE/s161/aws_status_check_2013-02-07+18.06.20.png"></p>

<p><img src="https://lh5.googleusercontent.com/-5avxLvp5RH8/URN6n-ihwiI/AAAAAAAAAso/zOHxli4vM8o/s702/aws_instance_check_2013-02-07+18.09.19.png"></p>

<p>と、期待して待っていたらなかなかアラートメールが来ません。。</p>

<p>設定間違ったかなーっていろいろ見直していたら20分経った頃にやっと到着。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Service: AWS Auto Scaling
</span><span class='line'>Time: 2013-02-07T09:17:42.304Z
</span><span class='line'>RequestId: f395660b-4847-4415-ad33-f8cc5091bdb3
</span><span class='line'>Event: autoscaling:EC2_INSTANCE_TERMINATE
</span><span class='line'>AccountId: 521026608000
</span><span class='line'>AutoScalingGroupName: test-sg
</span><span class='line'>AutoScalingGroupARN: arn:aws:autoscaling:ap-northeast-1:521026608000:autoScalingGroup:a036877b-dab7-4e5d-a6e1-1d3424b20d14:autoScalingGroupName/test-sg
</span><span class='line'>ActivityId: f395660b-4837-4415-ad33-f8cc5071bdb3
</span><span class='line'>Description: Terminating EC2 instance: i-15fadd17
</span><span class='line'>Cause: At 2013-02-07T09:16:57Z an instance was taken out of service in response to a system health-check.
</span><span class='line'>StartTime: 2013-02-07T09:16:57.389Z
</span><span class='line'>EndTime: 2013-02-07T09:17:42.304Z
</span><span class='line'>StatusCode: InProgress
</span><span class='line'>StatusMessage:
</span><span class='line'>Progress: 50
</span><span class='line'>EC2InstanceId: i-15fadd17
</span><span class='line'>Details: {}</span></code></pre></td></tr></table></div></figure>


<p>うーん。動く事は動いたけど、ちょっと時間がかかるなぁ。</p>

<p>この後何回か試してみたけど、Autoscaling発動までどれも20分ぐらいかかりました。</p>

<h3>結論</h3>

<p>20分程サーバダウンが許容できるようなゆるめの条件に限定した場合には非ELBでも使えるかな。まあ、それでも適用する場面は多々あるとは思いますが。（早める方法あるのかなー。）</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/12/03/cdp/">Auto Scalingの設定とデプロイ方法</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-12-03T01:34:00+09:00" pubdate data-updated="true">Dec 3<span>rd</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://www.zusaar.com/event/451061">CDP Advent Calendar 2012</a>に登録しました。ここ1年ちょいで使い慣れてきたパターンがあり、作った当時はクラウドデザインパターンはなかったのですが、<a href="http://aws.clouddesignpattern.org/index.php/CDP:Clone_Server%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3">Clone Server</a>と<a href="http://aws.clouddesignpattern.org/index.php/CDP:Scale_Out%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3">Scale Out</a>パターンの組合せに当てはまると思うので紹介します。ちなみにアプリはRails。</p>

<p>常にデプロイして更新し続けるシステムを手動、あるいは自動スケールアウトする時に便利な手法だったりします。</p>

<p>図にするとこんな感じですかね。</p>

<p><img src="https://lh4.googleusercontent.com/-eDSfSiz0XhU/ULyOlKj_JKI/AAAAAAAAArw/QzSfVJeeUfw/w727-h490-p-k/CDP_Clone%252BScale_Out.png"></p>

<h2>Auto Scaling設定</h2>

<h3>Launch Config</h3>

<p>まず、ベースとなるAMIの起動インスタンスサイズやセキュリティーグループを定義したLaunch Configを設定。</p>

<pre><code>as-create-launch-config cdp-lc --image-id ami-22a51d23 --instance-type m1.small \
--group cdp_web, cdp_admin
</code></pre>

<h3>Scaling Group</h3>

<p>次に、Scaling Groupで適用するAvailability Zone、インスタンス数の最小・最大閾値設定、紐尽くELB、死活監視方法・開始待ち時間、タグ等を定義。（ec2addtagではスペース入りのキーを設定できるのにas-create-auto-scaling-groupではできないので注意。早く直して。。）</p>

<pre><code>as-create-auto-scaling-group cdp-sg --launch-configuration cdp-lc \
--availability-zones ap-northeast-1a  --min-size 1 --max-size 9 --load-balancers cdp \
--health-check-type ELB --grace-period 300 --tag "k=Name, v=web.cdp" --tag "k=Use_Case, v=Test"
</code></pre>

<h3>Scaling Out Policy</h3>

<p>Scaling Outのポリシー設定。以下の例ではインスタンス1台を追加し、次のスケールアクションまでは5分間待機。ポリシーのARN (Amazon Resource Name)が出力されるので控えます。</p>

<pre><code>as-put-scaling-policy CDPOut --auto-scaling-group cdp-sg --adjustment 1 --type ChangeInCapacity \
--cooldown 300

&gt; arn:aws:autoscaling:ap-northeast-1:494850320039:scalingPolicy:d0d4dcf1-fb44-428a-a19c-38946633acf5:autoScalingGroupName/cdp-sg:policyName/CDPOut
</code></pre>

<h3>Cloudwatchトリガー（Scaling Out）</h3>

<p>Cloudwatchで閾値を超えたらスケールアウトするように設定。以下の例では対象Scaling Group配下のインスタンス達のCPUが1分間で平均75%以上で推移した場合、先ほど設定したScaling OutポリシーがARN経由で実行されます。また、閾値を超過あるいは下回った場合にSNS経由でアラートメールを飛ばします。</p>

<pre><code>mon-put-metric-alarm  CDPHighCPUAlarm --comparison-operator GreaterThanThreshold \
--evaluation-periods 1 --metric-name CPUUtilization --namespace "AWS/EC2" --period 60 \
--statistic Average --threshold 75 --dimensions "AutoScalingGroupName=cdp-sg" \
--ok-actions arn:aws:sns:ap-northeast-1:494850320039:cdp-alert --alarm-actions \
arn:aws:sns:ap-northeast-1:494850320039:cdp-alert, arn:aws:autoscaling:ap-northeast-1:494850320039:scalingPolicy:d0d4dcf1-fb44-428a-a19c-38946633acf5:autoScalingGroupName/cdp-sg:policyName/CDPOut
</code></pre>

<h3>Scaling Down Policy</h3>

<p>同じくScaling Downのポリシー設定。インスタンスが起動すると最低1時間は課金されるので頻繁に伸縮するともったいないのでスケールダウンアクションはゆっくりやるのがポイントです。</p>

<pre><code>as-put-scaling-policy CDPDown --auto-scaling-group cdp-sg --adjustment=-1 \
--type ChangeInCapacity --cooldown 1500

&gt; arn:aws:autoscaling:ap-northeast-1:494850320039:scalingPolicy:de53fbd5-130c-46a8-bf47-25e29f7d358e:autoScalingGroupName/cdp-sg:policyName/CDPDown
</code></pre>

<h3>Cloudwatchトリガー（Scaling Down）</h3>

<p>スケールダウンのトリガーは平均CPUが35%を25分間下回った場合に実行されます。この閾値周辺のアラートメールはいらないので設定してません。</p>

<pre><code>mon-put-metric-alarm CDPLowCPUAlarm --comparison-operator LessThanThreshold \
--evaluation-periods 1 --metric-name CPUUtilization --namespace "AWS/EC2" --period 1500 \
--statistic Average --threshold 35 --dimensions "AutoScalingGroupName=cdp-sg" --alarm-actions \
arn:aws:autoscaling:ap-northeast-1:494850320039:scalingPolicy:de53fbd5-130c-46a8-bf47-25e29f7d358e:autoScalingGroupName/cdp-sg:policyName/CDPDown
</code></pre>

<h3>通知</h3>

<p>最後にスケールアクション時に通知が飛ぶように設定。例えば、あるインスタンスが不調により反応がなく、ポリシーによってリプレースされる場合に飛んだりします。</p>

<pre><code>as-put-notification-configuration cdp-sg -t arn:aws:sns:ap-northeast-1:494850320039:cdp-alert \
-n autoscaling:EC2_INSTANCE_LAUNCH, autoscaling:EC2_INSTANCE_TERMINATE, \
autoscaling:EC2_INSTANCE_LAUNCH_ERROR, autoscaling:EC2_INSTANCE_TERMINATE_ERROR
</code></pre>

<h2>APP側の設定</h2>

<h3>ソースコード同期</h3>

<p>同期には起動時にupstart経由でrsyncを叩いて管理サーバから最新ソースを取ってきてサービスを再起動します。転送時の圧縮モードはarcfourが一番スループットが出たのでそれに。</p>

<div><script src='https://gist.github.com/4191986.js?file='></script>
<noscript><pre><code># sync source code
description &quot;Update source code&quot;

start on filesystem
stop on shutdown

task
console output

script
time sudo -u deploy rsync -av --delete -e 'ssh -c arcfour -i /home/deploy/.ssh/id_dsa -o StrictHostKeyChecking=no' deploy@admin.cdp:/usr/local/apps/project/ /usr/local/rails_apps/project &gt;&gt; /mnt/update.log 2&gt;&amp;1
/etc/init.d/unicorn upgrade &gt;&gt; /mnt/update.log 2&gt;&amp;1
/etc/init.d/nginx restart &gt;&gt; /mnt/update.log 2&gt;&amp;1
end script
</code></pre></noscript></div>


<h2>Capistrano</h2>

<p>デプロイはマスターサーバ（管理兼）上でCapistranoを実行し、ELB配下の生きているインスタンスに対して更新をかけます。deploy.rbに追加する記述は以下の通り。昔はec2 api toolsを直接呼んでパースしたりしてコードが長かったのですが、今は<a href="http://aws.amazon.com/sdkforruby/">AWS SDK for Ruby</a>があり、<a href="http://aws.amazon.com/iam/">IAM role</a>でinstance profileを設定すればわずか数行でできてしまいます！</p>

<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="nb">require</span> <span class="s1">&#39;aws-sdk&#39;</span>
</span><span class='line'><span class="no">AWS</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="ss">:ec2_endpoint</span> <span class="o">=&gt;</span> <span class="s1">&#39;ec2.ap-northeast-1.amazonaws.com&#39;</span><span class="p">,</span> <span class="ss">:elb_endpoint</span> <span class="o">=&gt;</span> <span class="s1">&#39;elasticloadbalancing.ap-northeast-1.amazonaws.com&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">instances</span> <span class="o">=</span> <span class="no">AWS</span><span class="o">::</span><span class="no">ELB</span><span class="o">.</span><span class="n">new</span><span class="o">.</span><span class="n">load_balancers</span><span class="o">[</span><span class="s1">&#39;cdp&#39;</span><span class="o">].</span><span class="n">instances</span><span class="o">.</span><span class="n">select</span><span class="p">{</span><span class="o">|</span><span class="n">i</span><span class="o">|</span> <span class="n">i</span><span class="o">.</span><span class="n">exists?</span> <span class="o">&amp;&amp;</span> <span class="n">i</span><span class="o">.</span><span class="n">elb_health</span><span class="o">[</span><span class="ss">:state</span><span class="o">]</span> <span class="o">==</span> <span class="s2">&quot;InService&quot;</span><span class="p">}</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="o">&amp;</span><span class="ss">:public_dns_name</span><span class="p">)</span>
</span><span class='line'><span class="n">servers</span> <span class="o">=</span> <span class="n">instances</span> <span class="o">&lt;&lt;</span> <span class="s2">&quot;localhost&quot;</span>
</span><span class='line'><span class="n">role</span> <span class="ss">:app</span> <span class="k">do</span> <span class="n">servers</span> <span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<h2>注意点</h2>

<ul>
<li>同期するファイル数が多すぎる（数十万個）と、チェックサム比較で時間がかかってしまい同期が終了する前にAuto Scalingの死活監視によってインスタンスがterminateされ、またlaunchされterminateされるという恐怖のスケールループ地獄に陥る。。。（これ、制限できないのかな）</li>
<li>あらかじめトラフィック増の時間帯が分かっていたら<a href="http://docs.amazonwebservices.com/AutoScaling/latest/DeveloperGuide/scaling_plan.html#schedule_time">Scheduled Action</a>で多めに設定しておいて自動スケールダウンさせた方が吉。</li>
<li>マスタサーバがSPOFとなりうるので冗長化する、もしくはrsyncよりs3経由にした方が良い。</li>
<li>急激なトラフィック増ではELB自体のスケールが追いつかない場合も。</li>
</ul>

</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2013/12/14/self-healing-with-non-elb-autoscaling2/">Autoscalingによる自動復旧(Immutable Infrastucture)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/13/serverfesta-2013-autumn/">サバフェス！2013に参加してきた</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/11/11/isucon3-final/">ISUCON3の本戦に参加してきた</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/10/07/isucon3-preliminary/">ISUCON3の予選を通過した（はず）</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/06/10/aws-game-day-tokyo-2013/">AWS Game Day Tokyo 2013で受賞してきた</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/05/21/custom-non-rds-multi-az-mysql-replication/">Non-RDSなオレオレMulti-AZ MySQL Replication</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/04/03/accelerating-cross-region-data-transfer/">リージョン間高速データ転送</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/02/22/mysql-benchmarks-on-aws-ssd-vs-fusion-io/">AWS SSD (hi1.4xlarge) vs Fusion-IOでのMySQLベンチマーク</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/02/08/self-healing-with-non-elb-autoscaling/">非ELBなAutoscalingによる自動復旧</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/12/03/cdp/">Auto Scalingの設定とデプロイ方法</a>
      </li>
    
  </ul>
</section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("ijin", 4, false);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/ijin" class="twitter-follow-button" data-show-count="false">Follow @ijin</a>
  
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Michael H. Oshita -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'ijin';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
